{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import floor\n",
    "import numpy as np\n",
    "\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training MNIST data\n",
    "train_data = datasets.MNIST(root='data', train=True, download=True, transform=ToTensor())\n",
    "\n",
    "# testing MNIST data\n",
    "test_data = datasets.MNIST(root='data', train=False, download=True, transform=ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([60000, 28, 28])\n",
      "torch.Size([60000])\n"
     ]
    }
   ],
   "source": [
    "# DEBUG CELLS\n",
    "print(type(train_data.data))\n",
    "print(train_data.data.shape)\n",
    "print(train_data.targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the network structure\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        super(Network, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_shape, 500)\n",
    "        self.fc2 = nn.Linear(500, 300)\n",
    "        self.output = nn.Linear(300, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = 784\n",
    "\n",
    "# training hyperparameters\n",
    "n_epoch = 2\n",
    "learning_rate = 0.001\n",
    "minibatch_sz = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the network, optimizer and define the loss function\n",
    "network = Network(input_shape)\n",
    "optimizer = optim.SGD(network.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We will perfom task-wise training. A single task comprises of two classes from the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "task1 = [0, 1]\n",
    "task2 = [2, 3]\n",
    "task3 = [4, 5]\n",
    "task4 = [6, 7]\n",
    "task5 = [8, 9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Separate training and testing samples from each task. This is easier to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "task1_tr_samples = torch.where(torch.bitwise_or(train_data.targets == task1[0], train_data.targets == task1[1]) == 1)[0]\n",
    "\n",
    "task2_tr_samples = torch.where(torch.bitwise_or(train_data.targets == task2[0], train_data.targets == task2[1]))[0]\n",
    "\n",
    "task3_tr_samples = torch.where(torch.bitwise_or(train_data.targets == task3[0], train_data.targets == task3[1]) == 1)[0]\n",
    "\n",
    "task4_tr_samples = torch.where(torch.bitwise_or(train_data.targets == task4[0], train_data.targets == task4[1]))[0]\n",
    "\n",
    "task5_tr_samples = torch.where(torch.bitwise_or(train_data.targets == task5[0], train_data.targets == task5[1]))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "task1_ts_samples = torch.where(torch.bitwise_or(test_data.targets == task1[0], test_data.targets == task1[1]) == 1)[0]\n",
    "\n",
    "task2_ts_samples = torch.where(torch.bitwise_or(test_data.targets == task2[0], test_data.targets == task2[1]))[0]\n",
    "\n",
    "task3_ts_samples = torch.where(torch.bitwise_or(test_data.targets == task3[0], test_data.targets == task3[1]) == 1)[0]\n",
    "\n",
    "task4_ts_samples = torch.where(torch.bitwise_or(test_data.targets == task4[0], test_data.targets == task4[1]))[0]\n",
    "\n",
    "task5_ts_samples = torch.where(torch.bitwise_or(test_data.targets == task5[0], test_data.targets == task5[1]))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Question 1**: The purpose of this question is to demonstrate the problem of catastrophic forgetting. For this purpose, we will train a single network on two different tasks in a sequence. After training evaluate the performance of the trained network on both tasks. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train on task 1\n",
    "# This code has been done for you as an example. Use the following code as a reference for subsequent codes you write.\n",
    "for e in range(n_epoch):\n",
    "    n_batch = floor(task1_tr_samples.shape[0] / minibatch_sz)\n",
    "    \n",
    "    for b in range(n_batch):\n",
    "        x_batch = train_data.data[task1_tr_samples[(b*minibatch_sz):((b+1)*minibatch_sz)]]\n",
    "        y_batch = train_data.targets[task1_tr_samples[(b*minibatch_sz):((b+1)*minibatch_sz)]]\n",
    "\n",
    "        # flatten image before presenting to the network and normalize intensities to the range [0, 1]\n",
    "        x_batch = torch.flatten(x_batch / 255, start_dim=1)\n",
    "\n",
    "        # convert label to one hot\n",
    "        y_batch = F.one_hot(y_batch).float()\n",
    "\n",
    "        y_hat_batch = network(x_batch)\n",
    "        loss = criterion(y_hat_batch, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f'Epoch {e}: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 99.57446808510639\n"
     ]
    }
   ],
   "source": [
    "# test on Task 1 and compute the model's classification accuracy. This cell is for debugging. It will let you know whether your model has trained well or not.\n",
    "\n",
    "# ADD YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train on task 2\n",
    "\n",
    "# ADD YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 90.26004728132388\n"
     ]
    }
   ],
   "source": [
    "# test on Task 2\n",
    "\n",
    "# ADD YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 97.58865248226951\n"
     ]
    }
   ],
   "source": [
    "# test on Task 1\n",
    "\n",
    "# ADD YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Question 2**: The purpose of this question is to study the effect of replay on catatophic forgetting. In this question also, we will train the network on two tasks in a sequence? When we train the network on the second task, we will also use some samples from the first task for replay. TO keep things simple, select a random proportaion (say 50%) of samples from the first task for replay. After training evaluate the performance of the trained network on both tasks. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save some samples from previous tasks for replay\n",
    "prop_saved = 0.5 # proportion of samples saved from a task for replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new network\n",
    "network = Network(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train on task 1. This is basically a copy of the code from above.\n",
    "for e in range(n_epoch):\n",
    "    n_batch = floor(task1_tr_samples.shape[0] / minibatch_sz)\n",
    "    \n",
    "    for b in range(n_batch):\n",
    "        x_batch = train_data.data[task1_tr_samples[(b*minibatch_sz):((b+1)*minibatch_sz)]]\n",
    "        y_batch = train_data.targets[task1_tr_samples[(b*minibatch_sz):((b+1)*minibatch_sz)]]\n",
    "\n",
    "        # flatten image before presenting to the network and normalize intensities to the range [0, 1]\n",
    "        x_batch = torch.flatten(x_batch / 255, start_dim=1)\n",
    "\n",
    "        # convert label to one hot\n",
    "        y_batch = F.one_hot(y_batch).float()\n",
    "\n",
    "        y_hat_batch = network(x_batch)\n",
    "        loss = criterion(y_hat_batch, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f'Epoch {e}: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select random samples from task 1 for replay.\n",
    "\n",
    "task1_replay = np.random.choice(task1_tr_samples.numpy(), int(prop_saved * task1_tr_samples.shape[0]))\n",
    "task1_replay_samples = torch.Tensor(task1_replay).int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train on task 2 with replay\n",
    "\n",
    "# ADD YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate your model's performance on both task 1 and 2.\n",
    "\n",
    "# ADD YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Directions for further exploration**\n",
    "We will not share solutions for these questions.\n",
    "\n",
    "**Q1**: How does the proportion of samples saved for replay affect the model's performance?\n",
    "\n",
    "**Q2**: Use replay to train the nentwork on more than two tasks. What is the impact of replay on the memory used by your models? Note that replay-based approach requires that you save the replay samples from previous task forever. This implies that the memory required to store samples contributes to your models memory footprint.\n",
    "\n",
    "**Q3**: Can we chose replay samples more smartly so that we generate maximal impact while using minimal memory? For instance, can you use the network's prediction on a given task to identify samples stored for replay?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
