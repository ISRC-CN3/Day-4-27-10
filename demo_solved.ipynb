{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import floor\n",
    "import numpy as np\n",
    "\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training MNIST data\n",
    "train_data = datasets.MNIST(root='data', train=True, download=True, transform=ToTensor())\n",
    "\n",
    "# testing MNIST data\n",
    "test_data = datasets.MNIST(root='data', train=False, download=True, transform=ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([60000, 28, 28])\n",
      "torch.Size([60000])\n"
     ]
    }
   ],
   "source": [
    "# DEBUG CELLS\n",
    "print(type(train_data.data))\n",
    "print(train_data.data.shape)\n",
    "print(train_data.targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the network structure\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        super(Network, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_shape, 500)\n",
    "        self.fc2 = nn.Linear(500, 300)\n",
    "        self.output = nn.Linear(300, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = 784\n",
    "\n",
    "# training hyperparameters\n",
    "n_epoch = 2\n",
    "learning_rate = 0.001\n",
    "minibatch_sz = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the network, optimizer and define the loss function\n",
    "network = Network(input_shape)\n",
    "optimizer = optim.SGD(network.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We will perfom task-wise training. A single task comprises of two classes from the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "task1 = [0, 1]\n",
    "task2 = [2, 3]\n",
    "task3 = [4, 5]\n",
    "task4 = [6, 7]\n",
    "task5 = [8, 9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Separate training and testing samples from each task. This is easier to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "task1_tr_samples = torch.where(torch.bitwise_or(train_data.targets == task1[0], train_data.targets == task1[1]) == 1)[0]\n",
    "\n",
    "task2_tr_samples = torch.where(torch.bitwise_or(train_data.targets == task2[0], train_data.targets == task2[1]))[0]\n",
    "\n",
    "task3_tr_samples = torch.where(torch.bitwise_or(train_data.targets == task3[0], train_data.targets == task3[1]) == 1)[0]\n",
    "\n",
    "task4_tr_samples = torch.where(torch.bitwise_or(train_data.targets == task4[0], train_data.targets == task4[1]))[0]\n",
    "\n",
    "task5_tr_samples = torch.where(torch.bitwise_or(train_data.targets == task5[0], train_data.targets == task5[1]))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "task1_ts_samples = torch.where(torch.bitwise_or(test_data.targets == task1[0], test_data.targets == task1[1]) == 1)[0]\n",
    "\n",
    "task2_ts_samples = torch.where(torch.bitwise_or(test_data.targets == task2[0], test_data.targets == task2[1]))[0]\n",
    "\n",
    "task3_ts_samples = torch.where(torch.bitwise_or(test_data.targets == task3[0], test_data.targets == task3[1]) == 1)[0]\n",
    "\n",
    "task4_ts_samples = torch.where(torch.bitwise_or(test_data.targets == task4[0], test_data.targets == task4[1]))[0]\n",
    "\n",
    "task5_ts_samples = torch.where(torch.bitwise_or(test_data.targets == task5[0], test_data.targets == task5[1]))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Question 1**: The purpose of this question is to demonstrate the problem of catastrophic forgetting. For this purpose, we will train a single network on two different tasks in a sequence. After training evaluate the performance of the trained network on both tasks. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 0.7139849662780762\n",
      "Epoch 0: 0.7199063301086426\n",
      "Epoch 0: 0.7150048613548279\n",
      "Epoch 0: 0.7135699987411499\n",
      "Epoch 0: 0.716407060623169\n",
      "Epoch 0: 0.7134637236595154\n",
      "Epoch 0: 0.7109566330909729\n",
      "Epoch 0: 0.7090693116188049\n",
      "Epoch 0: 0.7071647047996521\n",
      "Epoch 0: 0.7027407288551331\n",
      "Epoch 0: 0.7034420967102051\n",
      "Epoch 0: 0.6973006129264832\n",
      "Epoch 0: 0.700343668460846\n",
      "Epoch 0: 0.6927923560142517\n",
      "Epoch 0: 0.6857284903526306\n",
      "Epoch 0: 0.6867830157279968\n",
      "Epoch 0: 0.6825870871543884\n",
      "Epoch 0: 0.6698495149612427\n",
      "Epoch 0: 0.6691404581069946\n",
      "Epoch 0: 0.6729548573493958\n",
      "Epoch 0: 0.6643473505973816\n",
      "Epoch 0: 0.6586947441101074\n",
      "Epoch 0: 0.6530790328979492\n",
      "Epoch 0: 0.6563106179237366\n",
      "Epoch 0: 0.6547855734825134\n",
      "Epoch 0: 0.6528515815734863\n",
      "Epoch 0: 0.6237528920173645\n",
      "Epoch 0: 0.6257097721099854\n",
      "Epoch 0: 0.6341657042503357\n",
      "Epoch 0: 0.6157877445220947\n",
      "Epoch 0: 0.6068785786628723\n",
      "Epoch 0: 0.6189902424812317\n",
      "Epoch 0: 0.5914162993431091\n",
      "Epoch 0: 0.5930476784706116\n",
      "Epoch 0: 0.5774946212768555\n",
      "Epoch 0: 0.5772671699523926\n",
      "Epoch 0: 0.5642006993293762\n",
      "Epoch 0: 0.5690854787826538\n",
      "Epoch 0: 0.5577806234359741\n",
      "Epoch 0: 0.5356327295303345\n",
      "Epoch 0: 0.5509772300720215\n",
      "Epoch 0: 0.5463916659355164\n",
      "Epoch 0: 0.5257998108863831\n",
      "Epoch 0: 0.507414698600769\n",
      "Epoch 0: 0.5013466477394104\n",
      "Epoch 0: 0.48485153913497925\n",
      "Epoch 0: 0.49590003490448\n",
      "Epoch 0: 0.4524099826812744\n",
      "Epoch 0: 0.48616206645965576\n",
      "Epoch 0: 0.4548611044883728\n",
      "Epoch 0: 0.45411965250968933\n",
      "Epoch 0: 0.4225904941558838\n",
      "Epoch 0: 0.4219183921813965\n",
      "Epoch 0: 0.38604938983917236\n",
      "Epoch 0: 0.3773699700832367\n",
      "Epoch 0: 0.37011346220970154\n",
      "Epoch 0: 0.3666492998600006\n",
      "Epoch 0: 0.3411099314689636\n",
      "Epoch 0: 0.3788147568702698\n",
      "Epoch 0: 0.32018423080444336\n",
      "Epoch 0: 0.276853084564209\n",
      "Epoch 0: 0.2686920762062073\n",
      "Epoch 0: 0.25670358538627625\n",
      "Epoch 0: 0.25474846363067627\n",
      "Epoch 0: 0.187043696641922\n",
      "Epoch 0: 0.17815150320529938\n",
      "Epoch 0: 0.20527052879333496\n",
      "Epoch 0: 0.19829005002975464\n",
      "Epoch 0: 0.1415909081697464\n",
      "Epoch 0: 0.12098365277051926\n",
      "Epoch 0: 0.150197833776474\n",
      "Epoch 0: 0.130763441324234\n",
      "Epoch 0: 0.11217740178108215\n",
      "Epoch 0: 0.1251348853111267\n",
      "Epoch 0: 0.07383306324481964\n",
      "Epoch 0: 0.06493297219276428\n",
      "Epoch 0: 0.06504231691360474\n",
      "Epoch 0: 0.06207049638032913\n",
      "Epoch 0: 0.06304804980754852\n",
      "Epoch 0: 0.046236056834459305\n",
      "Epoch 0: 0.03763698786497116\n",
      "Epoch 0: 0.041011564433574677\n",
      "Epoch 0: 0.027512632310390472\n",
      "Epoch 0: 0.050127048045396805\n",
      "Epoch 0: 0.017874913290143013\n",
      "Epoch 0: 0.03727852553129196\n",
      "Epoch 0: 0.02129509299993515\n",
      "Epoch 0: 0.07126928865909576\n",
      "Epoch 0: 0.011950800195336342\n",
      "Epoch 0: 0.005630799103528261\n",
      "Epoch 0: 0.010075856000185013\n",
      "Epoch 0: 0.009138301014900208\n",
      "Epoch 0: 0.044453416019678116\n",
      "Epoch 0: 0.003896396141499281\n",
      "Epoch 0: 0.12619081139564514\n",
      "Epoch 0: 0.0035923297982662916\n",
      "Epoch 0: 0.004149340093135834\n",
      "Epoch 0: 0.04107501357793808\n",
      "Epoch 0: 0.006165968254208565\n",
      "Epoch 0: 0.005114113911986351\n",
      "Epoch 0: 0.07222910970449448\n",
      "Epoch 0: 0.001065135933458805\n",
      "Epoch 0: 0.002443372504785657\n",
      "Epoch 0: 0.05456416681408882\n",
      "Epoch 0: 0.02833184413611889\n",
      "Epoch 0: 0.005226399749517441\n",
      "Epoch 0: 0.001169149880297482\n",
      "Epoch 0: 0.0002107456384692341\n",
      "Epoch 0: 0.004702837206423283\n",
      "Epoch 0: 0.001093535334803164\n",
      "Epoch 0: 0.0007640303811058402\n",
      "Epoch 0: 0.0019929760601371527\n",
      "Epoch 0: 0.0008799452916719019\n",
      "Epoch 0: 0.005017736926674843\n",
      "Epoch 0: 0.0024160067550837994\n",
      "Epoch 0: 0.010822500102221966\n",
      "Epoch 0: 6.263571413001046e-05\n",
      "Epoch 0: 2.285796290379949e-05\n",
      "Epoch 0: 0.00020238447177689523\n",
      "Epoch 0: 0.005408382974565029\n",
      "Epoch 0: 1.1099699804617558e-05\n",
      "Epoch 0: 0.0018701332155615091\n",
      "Epoch 0: 0.0007702005095779896\n",
      "Epoch 0: 0.0006810491904616356\n",
      "Epoch 0: 0.0017701395554468036\n",
      "Epoch 0: 0.07140866667032242\n",
      "Epoch 0: 2.16458756767679e-05\n",
      "Epoch 0: 0.2399672567844391\n",
      "Epoch 0: 2.7826511086459504e-06\n",
      "Epoch 0: 9.643770681577735e-06\n",
      "Epoch 0: 0.0010026117088273168\n",
      "Epoch 0: 0.1380843222141266\n",
      "Epoch 0: 5.401615226219292e-07\n",
      "Epoch 0: 0.20084136724472046\n",
      "Epoch 0: 0.11964113265275955\n",
      "Epoch 0: 6.891775683470769e-08\n",
      "Epoch 0: 1.3727240002481267e-06\n",
      "Epoch 0: 0.0012007270706817508\n",
      "Epoch 0: 3.345078994243522e-06\n",
      "Epoch 0: 1.5273653275471588e-07\n",
      "Epoch 0: 0.10171474516391754\n",
      "Epoch 0: 9.313224857976365e-09\n",
      "Epoch 0: 3.892884308243083e-07\n",
      "Epoch 0: 1.8626450382086546e-09\n",
      "Epoch 0: -0.0\n",
      "Epoch 0: 0.11277446150779724\n",
      "Epoch 0: 7.897438081272412e-07\n",
      "Epoch 0: 7.636833032620416e-08\n",
      "Epoch 0: -0.0\n",
      "Epoch 0: -0.0\n",
      "Epoch 0: 0.025906044989824295\n",
      "Epoch 0: -0.0\n",
      "Epoch 0: 0.0003128056414425373\n",
      "Epoch 0: -0.0\n",
      "Epoch 0: 3.873903096973663e-06\n",
      "Epoch 0: 2.8607610147446394e-06\n",
      "Epoch 0: 1.8626450382086546e-09\n",
      "Epoch 0: 3.2409690220447374e-07\n",
      "Epoch 0: -0.0\n",
      "Epoch 0: 2.6077010772951326e-08\n",
      "Epoch 0: -0.0\n",
      "Epoch 0: -0.0\n",
      "Epoch 0: 0.004880952648818493\n",
      "Epoch 0: -0.0\n",
      "Epoch 0: 8.381891092312799e-08\n",
      "Epoch 0: -0.0\n",
      "Epoch 0: 0.27729570865631104\n",
      "Epoch 0: 0.5532320141792297\n",
      "Epoch 0: -0.0\n",
      "Epoch 0: -0.0\n",
      "Epoch 0: 9.594299626769498e-05\n",
      "Epoch 0: -0.0\n",
      "Epoch 0: 2.4214378058218244e-08\n",
      "Epoch 0: 0.1262694150209427\n",
      "Epoch 0: 1.1845668268506415e-05\n",
      "Epoch 0: -0.0\n",
      "Epoch 0: -0.0\n",
      "Epoch 0: -0.0\n",
      "Epoch 0: -0.0\n",
      "Epoch 0: 4.097813999237587e-08\n",
      "Epoch 0: -0.0\n",
      "Epoch 0: -0.0\n",
      "Epoch 0: -0.0\n",
      "Epoch 0: -0.0\n",
      "Epoch 0: 3.953673513024114e-05\n",
      "Epoch 0: -0.0\n",
      "Epoch 0: -0.0\n",
      "Epoch 0: -0.0\n",
      "Epoch 0: -0.0\n",
      "Epoch 0: -0.0\n",
      "Epoch 0: -0.0\n",
      "Epoch 0: -0.0\n",
      "Epoch 0: -0.0\n",
      "Epoch 0: 0.03738958016037941\n",
      "Epoch 0: -0.0\n",
      "Epoch 0: -0.0\n",
      "Epoch 0: -0.0\n",
      "Epoch 1: 0.6236628890037537\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: 0.7019559144973755\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: 0.9760916233062744\n",
      "Epoch 1: 0.49260038137435913\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: 1.1752605132642202e-05\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: 1.2915570735931396\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: 0.28177765011787415\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: 1.1901496691280045e-05\n",
      "Epoch 1: 0.05440164729952812\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: 0.9716196060180664\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: 0.5505678653717041\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: 0.33432382345199585\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: 0.0008333282894454896\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: 1.8626450382086546e-09\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: 2.5738987922668457\n",
      "Epoch 1: 0.937359094619751\n",
      "Epoch 1: 1.3185762166976929\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: 1.145840048789978\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: 0.1782766878604889\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: 0.8208507299423218\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: 3.7795310020446777\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: 0.027427323162555695\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: 0.5807405710220337\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: 0.7375516891479492\n",
      "Epoch 1: 0.9400335550308228\n",
      "Epoch 1: 6.8102090153843164e-06\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: 0.3109031915664673\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: 0.4368162751197815\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: 0.0008260016329586506\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: 3.123530387878418\n",
      "Epoch 1: 0.30997973680496216\n",
      "Epoch 1: 0.06513135880231857\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: 0.21464930474758148\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: 0.3778665065765381\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: 0.3831231892108917\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: 1.8626450382086546e-09\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: 0.7243131995201111\n",
      "Epoch 1: 2.3210344314575195\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n"
     ]
    }
   ],
   "source": [
    "# train on task 1\n",
    "for e in range(n_epoch):\n",
    "    n_batch = floor(task1_tr_samples.shape[0] / minibatch_sz)\n",
    "    \n",
    "    for b in range(n_batch):\n",
    "        x_batch = train_data.data[task1_tr_samples[(b*minibatch_sz):((b+1)*minibatch_sz)]]\n",
    "        y_batch = train_data.targets[task1_tr_samples[(b*minibatch_sz):((b+1)*minibatch_sz)]]\n",
    "\n",
    "        # flatten image before presenting to the network and normalize intensities to the range [0, 1]\n",
    "        x_batch = torch.flatten(x_batch / 255, start_dim=1)\n",
    "\n",
    "        # convert label to one hot\n",
    "        y_batch = F.one_hot(y_batch).float()\n",
    "\n",
    "        y_hat_batch = network(x_batch)\n",
    "        loss = criterion(y_hat_batch, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f'Epoch {e}: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 99.81087470449172\n"
     ]
    }
   ],
   "source": [
    "# test on Task 1\n",
    "n_batch = floor(task1_ts_samples.shape[0] / minibatch_sz)\n",
    "\n",
    "n_correct = 0    \n",
    "for b in range(n_batch):\n",
    "    x_batch = test_data.data[task1_ts_samples[(b*minibatch_sz):((b+1)*minibatch_sz)]]\n",
    "    y_batch = test_data.targets[task1_ts_samples[(b*minibatch_sz):((b+1)*minibatch_sz)]]\n",
    "\n",
    "    # flatten image before presenting to the network and normalize intensities to the range [0, 1]\n",
    "    x_batch = torch.flatten(x_batch / 255, start_dim=1)\n",
    "\n",
    "    y_hat_batch = network(x_batch)\n",
    "    _, prediction = torch.max(y_hat_batch, 1)\n",
    "    n_correct += (prediction == y_batch).sum().item()\n",
    "\n",
    "print(f'Accuracy = {(n_correct * 100) / task1_ts_samples.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 69.4424057006836\n",
      "Epoch 0: 77.93883514404297\n",
      "Epoch 0: 95.78223419189453\n",
      "Epoch 0: 77.31819152832031\n",
      "Epoch 0: 82.10861206054688\n",
      "Epoch 0: 97.94888305664062\n",
      "Epoch 0: 47.53200912475586\n",
      "Epoch 0: 41.89323425292969\n",
      "Epoch 0: 37.094451904296875\n",
      "Epoch 0: 24.8565616607666\n",
      "Epoch 0: 10.43249797821045\n",
      "Epoch 0: 17.543197631835938\n",
      "Epoch 0: 30.247007369995117\n",
      "Epoch 0: 20.887351989746094\n",
      "Epoch 0: 14.361005783081055\n",
      "Epoch 0: 5.889129638671875\n",
      "Epoch 0: 3.509218692779541\n",
      "Epoch 0: 1.243325114250183\n",
      "Epoch 0: 3.5031096935272217\n",
      "Epoch 0: 13.644874572753906\n",
      "Epoch 0: 18.154708862304688\n",
      "Epoch 0: 17.467178344726562\n",
      "Epoch 0: 17.80025863647461\n",
      "Epoch 0: 5.460891246795654\n",
      "Epoch 0: 6.295725345611572\n",
      "Epoch 0: 7.896490097045898\n",
      "Epoch 0: 5.619497299194336\n",
      "Epoch 0: 23.22791290283203\n",
      "Epoch 0: 14.744108200073242\n",
      "Epoch 0: 29.070405960083008\n",
      "Epoch 0: 24.24897575378418\n",
      "Epoch 0: 9.862651824951172\n",
      "Epoch 0: 4.7876176834106445\n",
      "Epoch 0: 1.9906792640686035\n",
      "Epoch 0: 2.841184139251709\n",
      "Epoch 0: 3.460263729095459\n",
      "Epoch 0: 11.098705291748047\n",
      "Epoch 0: 4.0073089599609375\n",
      "Epoch 0: 7.802928924560547\n",
      "Epoch 0: 9.9701566696167\n",
      "Epoch 0: 4.796608924865723\n",
      "Epoch 0: 1.5117906332015991\n",
      "Epoch 0: 1.4817259311676025\n",
      "Epoch 0: 1.7666618824005127\n",
      "Epoch 0: 1.686871886253357\n",
      "Epoch 0: 6.057578086853027\n",
      "Epoch 0: 3.4891233444213867\n",
      "Epoch 0: 1.5314924716949463\n",
      "Epoch 0: 2.4952406883239746\n",
      "Epoch 0: 0.692668616771698\n",
      "Epoch 0: 1.1287734508514404\n",
      "Epoch 0: 1.3582314252853394\n",
      "Epoch 0: 0.3164029121398926\n",
      "Epoch 0: 0.16836094856262207\n",
      "Epoch 0: 0.2829172909259796\n",
      "Epoch 0: 1.4891934394836426\n",
      "Epoch 0: 0.6863005757331848\n",
      "Epoch 0: 0.9365524053573608\n",
      "Epoch 0: 1.8184977769851685\n",
      "Epoch 0: 0.8178543448448181\n",
      "Epoch 0: 5.360745906829834\n",
      "Epoch 0: 5.6171064376831055\n",
      "Epoch 0: 1.3494770526885986\n",
      "Epoch 0: 0.027776405215263367\n",
      "Epoch 0: 0.010763488709926605\n",
      "Epoch 0: 4.056512355804443\n",
      "Epoch 0: 0.49844613671302795\n",
      "Epoch 0: 1.9216171503067017\n",
      "Epoch 0: 2.106203317642212\n",
      "Epoch 0: 7.107839584350586\n",
      "Epoch 0: 1.1588424444198608\n",
      "Epoch 0: 5.511038780212402\n",
      "Epoch 0: 0.605642557144165\n",
      "Epoch 0: 0.13210034370422363\n",
      "Epoch 0: 3.528502941131592\n",
      "Epoch 0: 4.660971641540527\n",
      "Epoch 0: 2.558102607727051\n",
      "Epoch 0: 0.5790127515792847\n",
      "Epoch 0: 2.204047918319702\n",
      "Epoch 0: 3.646371603012085\n",
      "Epoch 0: 2.1772186756134033\n",
      "Epoch 0: 7.66729211807251\n",
      "Epoch 0: -0.0\n",
      "Epoch 0: 7.977108001708984\n",
      "Epoch 0: 9.004968643188477\n",
      "Epoch 0: 18.444759368896484\n",
      "Epoch 0: 23.435636520385742\n",
      "Epoch 0: 10.838014602661133\n",
      "Epoch 0: 11.217324256896973\n",
      "Epoch 0: 3.73712158203125\n",
      "Epoch 0: 2.319797992706299\n",
      "Epoch 0: 10.035719871520996\n",
      "Epoch 0: 2.8975741863250732\n",
      "Epoch 0: 9.098228454589844\n",
      "Epoch 0: 5.799684524536133\n",
      "Epoch 0: 0.05331437662243843\n",
      "Epoch 0: 15.605460166931152\n",
      "Epoch 0: 9.314239501953125\n",
      "Epoch 0: 5.777814865112305\n",
      "Epoch 0: 5.99899435043335\n",
      "Epoch 0: 3.8565433025360107\n",
      "Epoch 0: 5.709441184997559\n",
      "Epoch 0: 2.1283624172210693\n",
      "Epoch 0: 0.009823327884078026\n",
      "Epoch 0: 7.78519344329834\n",
      "Epoch 0: 4.23297643661499\n",
      "Epoch 0: 3.068868398666382\n",
      "Epoch 0: 0.7793741226196289\n",
      "Epoch 0: 2.3447792530059814\n",
      "Epoch 0: 2.975701332092285\n",
      "Epoch 0: 0.6948989629745483\n",
      "Epoch 0: 0.11109288036823273\n",
      "Epoch 0: 1.0856982469558716\n",
      "Epoch 0: 4.461090087890625\n",
      "Epoch 0: 0.16761523485183716\n",
      "Epoch 0: 0.7364220023155212\n",
      "Epoch 0: 6.4785590171813965\n",
      "Epoch 0: 12.190407752990723\n",
      "Epoch 0: 1.0866738557815552\n",
      "Epoch 0: 6.747951030731201\n",
      "Epoch 0: 2.582832098007202\n",
      "Epoch 0: 2.9633727073669434\n",
      "Epoch 0: 3.9208014011383057\n",
      "Epoch 0: 1.9487721920013428\n",
      "Epoch 0: 1.2541863918304443\n",
      "Epoch 0: 1.1782665252685547\n",
      "Epoch 0: 1.0419867038726807\n",
      "Epoch 0: 0.3739210367202759\n",
      "Epoch 0: 0.743391752243042\n",
      "Epoch 0: 4.9325056076049805\n",
      "Epoch 0: 4.1829047203063965\n",
      "Epoch 0: 0.20577432215213776\n",
      "Epoch 0: 4.222641468048096\n",
      "Epoch 0: 3.6464364528656006\n",
      "Epoch 0: 7.524965286254883\n",
      "Epoch 0: 4.880908489227295\n",
      "Epoch 0: 2.2763726711273193\n",
      "Epoch 0: 3.361036539077759\n",
      "Epoch 0: 1.0493171215057373\n",
      "Epoch 0: 4.768668174743652\n",
      "Epoch 0: 7.561740875244141\n",
      "Epoch 0: 3.3592188358306885\n",
      "Epoch 0: 0.991581916809082\n",
      "Epoch 0: 0.7199098467826843\n",
      "Epoch 0: 4.857464790344238\n",
      "Epoch 0: 5.09544563293457\n",
      "Epoch 0: 0.3140357434749603\n",
      "Epoch 0: 1.1780729293823242\n",
      "Epoch 0: 1.4162880182266235\n",
      "Epoch 0: 2.8094003200531006\n",
      "Epoch 0: 0.03413637354969978\n",
      "Epoch 0: 2.7607688903808594\n",
      "Epoch 0: 0.9903377890586853\n",
      "Epoch 0: 0.7926287651062012\n",
      "Epoch 0: 0.4395253658294678\n",
      "Epoch 0: 1.8707435131072998\n",
      "Epoch 0: 8.681180953979492\n",
      "Epoch 0: 4.044151306152344\n",
      "Epoch 0: 5.771655559539795\n",
      "Epoch 0: 2.3693695068359375\n",
      "Epoch 0: 0.7053410410881042\n",
      "Epoch 0: 0.6105311512947083\n",
      "Epoch 0: 2.2795586585998535\n",
      "Epoch 0: 5.46056604385376\n",
      "Epoch 0: 1.24547278881073\n",
      "Epoch 0: 0.7327058911323547\n",
      "Epoch 0: 0.8956755995750427\n",
      "Epoch 0: 4.987610816955566\n",
      "Epoch 0: 5.756568908691406\n",
      "Epoch 0: 0.015137208625674248\n",
      "Epoch 0: 0.23027558624744415\n",
      "Epoch 0: 0.6790589690208435\n",
      "Epoch 0: 6.1380462646484375\n",
      "Epoch 0: 19.990768432617188\n",
      "Epoch 0: 9.621687889099121\n",
      "Epoch 0: 4.15241003036499\n",
      "Epoch 0: 0.36302655935287476\n",
      "Epoch 0: 6.457055568695068\n",
      "Epoch 0: 1.3195679187774658\n",
      "Epoch 0: 0.700144350528717\n",
      "Epoch 0: 2.3796212673187256\n",
      "Epoch 0: 13.452588081359863\n",
      "Epoch 0: 2.1021223068237305\n",
      "Epoch 0: 0.3053704500198364\n",
      "Epoch 0: 0.0022179733496159315\n",
      "Epoch 0: 3.1204657554626465\n",
      "Epoch 0: 5.606308937072754\n",
      "Epoch 0: 6.7074875831604\n",
      "Epoch 1: 0.18550989031791687\n",
      "Epoch 1: 6.343835830688477\n",
      "Epoch 1: 10.485910415649414\n",
      "Epoch 1: 14.694540023803711\n",
      "Epoch 1: 13.47745132446289\n",
      "Epoch 1: 1.545736312866211\n",
      "Epoch 1: 4.81941556930542\n",
      "Epoch 1: 14.180402755737305\n",
      "Epoch 1: 4.7973408699035645\n",
      "Epoch 1: 2.4232072830200195\n",
      "Epoch 1: 3.2836142054293305e-05\n",
      "Epoch 1: 2.801234006881714\n",
      "Epoch 1: 0.39572426676750183\n",
      "Epoch 1: 3.7589878047583625e-05\n",
      "Epoch 1: 4.36387300491333\n",
      "Epoch 1: 13.406698226928711\n",
      "Epoch 1: 1.8415507078170776\n",
      "Epoch 1: 0.6132586002349854\n",
      "Epoch 1: 0.7272816300392151\n",
      "Epoch 1: 0.865464448928833\n",
      "Epoch 1: 17.06135368347168\n",
      "Epoch 1: 6.206249237060547\n",
      "Epoch 1: 5.761631011962891\n",
      "Epoch 1: 6.781952857971191\n",
      "Epoch 1: 5.849143028259277\n",
      "Epoch 1: 5.609685897827148\n",
      "Epoch 1: 8.014863967895508\n",
      "Epoch 1: 3.3521454334259033\n",
      "Epoch 1: 18.78272247314453\n",
      "Epoch 1: 0.771324634552002\n",
      "Epoch 1: 1.8441787958145142\n",
      "Epoch 1: 2.0455925464630127\n",
      "Epoch 1: 0.7947491407394409\n",
      "Epoch 1: 0.12212807685136795\n",
      "Epoch 1: 0.2579424977302551\n",
      "Epoch 1: 12.798742294311523\n",
      "Epoch 1: 51.781002044677734\n",
      "Epoch 1: 15.529773712158203\n",
      "Epoch 1: 17.21603012084961\n",
      "Epoch 1: 0.9922500252723694\n",
      "Epoch 1: 5.359869480133057\n",
      "Epoch 1: 5.7628374099731445\n",
      "Epoch 1: 3.483292818069458\n",
      "Epoch 1: 19.2375545501709\n",
      "Epoch 1: 2.0811033248901367\n",
      "Epoch 1: 8.99696159362793\n",
      "Epoch 1: 0.20252591371536255\n",
      "Epoch 1: 0.0765189528465271\n",
      "Epoch 1: 0.3287826478481293\n",
      "Epoch 1: 0.5557863116264343\n",
      "Epoch 1: 0.48079127073287964\n",
      "Epoch 1: 0.5584477782249451\n",
      "Epoch 1: 0.30256032943725586\n",
      "Epoch 1: 5.774615287780762\n",
      "Epoch 1: 0.42637789249420166\n",
      "Epoch 1: 3.478776454925537\n",
      "Epoch 1: 0.32907941937446594\n",
      "Epoch 1: 0.14532876014709473\n",
      "Epoch 1: 1.0511958599090576\n",
      "Epoch 1: 0.18781082332134247\n",
      "Epoch 1: 0.12251169979572296\n",
      "Epoch 1: 1.113804817199707\n",
      "Epoch 1: 0.11700049042701721\n",
      "Epoch 1: 0.14650529623031616\n",
      "Epoch 1: 0.12044225633144379\n",
      "Epoch 1: 6.557495594024658\n",
      "Epoch 1: 0.3447611331939697\n",
      "Epoch 1: 0.4229932725429535\n",
      "Epoch 1: 0.454358845949173\n",
      "Epoch 1: 0.8423051238059998\n",
      "Epoch 1: 0.36217767000198364\n",
      "Epoch 1: 0.16989167034626007\n",
      "Epoch 1: 0.2993526756763458\n",
      "Epoch 1: 0.17589712142944336\n",
      "Epoch 1: 4.870561122894287\n",
      "Epoch 1: 0.4724394381046295\n",
      "Epoch 1: 2.484733819961548\n",
      "Epoch 1: 0.18967409431934357\n",
      "Epoch 1: 0.1848268210887909\n",
      "Epoch 1: 0.5792534351348877\n",
      "Epoch 1: 0.2108767181634903\n",
      "Epoch 1: 0.4918750822544098\n",
      "Epoch 1: 7.3215678639826365e-06\n",
      "Epoch 1: 21.295557022094727\n",
      "Epoch 1: 0.4589495062828064\n",
      "Epoch 1: 1.1176437139511108\n",
      "Epoch 1: 11.592706680297852\n",
      "Epoch 1: 0.7638559937477112\n",
      "Epoch 1: 9.642143249511719\n",
      "Epoch 1: 4.027234077453613\n",
      "Epoch 1: 0.1580272912979126\n",
      "Epoch 1: 1.3239136934280396\n",
      "Epoch 1: 0.19183672964572906\n",
      "Epoch 1: 0.24832655489444733\n",
      "Epoch 1: 0.5048075318336487\n",
      "Epoch 1: 0.14148473739624023\n",
      "Epoch 1: 7.8482208251953125\n",
      "Epoch 1: 0.29160723090171814\n",
      "Epoch 1: 14.393205642700195\n",
      "Epoch 1: 0.3134489059448242\n",
      "Epoch 1: 0.081710085272789\n",
      "Epoch 1: 0.35827600955963135\n",
      "Epoch 1: 0.003614664077758789\n",
      "Epoch 1: 0.1969883143901825\n",
      "Epoch 1: 0.25744757056236267\n",
      "Epoch 1: 0.13363036513328552\n",
      "Epoch 1: 13.917709350585938\n",
      "Epoch 1: 0.4282616674900055\n",
      "Epoch 1: 0.4834837317466736\n",
      "Epoch 1: 5.266978740692139\n",
      "Epoch 1: 0.21940840780735016\n",
      "Epoch 1: 0.3178858757019043\n",
      "Epoch 1: 0.08001591265201569\n",
      "Epoch 1: 0.6669856309890747\n",
      "Epoch 1: 0.2834853529930115\n",
      "Epoch 1: 0.2388881891965866\n",
      "Epoch 1: 0.5871503949165344\n",
      "Epoch 1: 2.4288899898529053\n",
      "Epoch 1: 0.12169788032770157\n",
      "Epoch 1: 0.09119875729084015\n",
      "Epoch 1: 0.07933133095502853\n",
      "Epoch 1: 0.04558376222848892\n",
      "Epoch 1: 0.19901739060878754\n",
      "Epoch 1: 0.13060910999774933\n",
      "Epoch 1: 0.017395183444023132\n",
      "Epoch 1: 0.12142924964427948\n",
      "Epoch 1: 0.11651859432458878\n",
      "Epoch 1: 0.14237207174301147\n",
      "Epoch 1: 0.05553290992975235\n",
      "Epoch 1: 0.5004231929779053\n",
      "Epoch 1: 0.3905903398990631\n",
      "Epoch 1: 0.2234179675579071\n",
      "Epoch 1: 0.6925899386405945\n",
      "Epoch 1: 0.5297501683235168\n",
      "Epoch 1: 0.4596485495567322\n",
      "Epoch 1: 0.28424057364463806\n",
      "Epoch 1: 0.15704600512981415\n",
      "Epoch 1: 0.24839738011360168\n",
      "Epoch 1: 0.0823783352971077\n",
      "Epoch 1: 1.1424305438995361\n",
      "Epoch 1: 11.466814041137695\n",
      "Epoch 1: 0.17635683715343475\n",
      "Epoch 1: 0.07341094315052032\n",
      "Epoch 1: 0.12085623294115067\n",
      "Epoch 1: 0.34078744053840637\n",
      "Epoch 1: 4.972507953643799\n",
      "Epoch 1: 0.12967978417873383\n",
      "Epoch 1: 0.03680480644106865\n",
      "Epoch 1: 0.06709056347608566\n",
      "Epoch 1: 0.20389625430107117\n",
      "Epoch 1: 0.18119096755981445\n",
      "Epoch 1: 0.4137367308139801\n",
      "Epoch 1: 0.12915417551994324\n",
      "Epoch 1: 0.2938691973686218\n",
      "Epoch 1: 0.08877047896385193\n",
      "Epoch 1: 0.13861429691314697\n",
      "Epoch 1: 1.0194307565689087\n",
      "Epoch 1: 0.46711432933807373\n",
      "Epoch 1: 1.0271461009979248\n",
      "Epoch 1: 0.15726545453071594\n",
      "Epoch 1: 0.1647929847240448\n",
      "Epoch 1: 0.2990770936012268\n",
      "Epoch 1: 0.0986018106341362\n",
      "Epoch 1: 0.1221763864159584\n",
      "Epoch 1: 0.22775696218013763\n",
      "Epoch 1: 0.06728905439376831\n",
      "Epoch 1: 0.07299232482910156\n",
      "Epoch 1: 2.8478176593780518\n",
      "Epoch 1: 0.2065175175666809\n",
      "Epoch 1: 0.022810298949480057\n",
      "Epoch 1: 0.01575876772403717\n",
      "Epoch 1: 0.10488729923963547\n",
      "Epoch 1: 0.19024860858917236\n",
      "Epoch 1: 0.40218523144721985\n",
      "Epoch 1: 0.20233012735843658\n",
      "Epoch 1: 0.04630770906805992\n",
      "Epoch 1: 0.01423470675945282\n",
      "Epoch 1: 0.1344040483236313\n",
      "Epoch 1: 0.06600738316774368\n",
      "Epoch 1: 0.06584619730710983\n",
      "Epoch 1: 0.03127877041697502\n",
      "Epoch 1: 0.2417217493057251\n",
      "Epoch 1: 0.08628608286380768\n",
      "Epoch 1: 0.17692160606384277\n",
      "Epoch 1: 0.005983311217278242\n",
      "Epoch 1: 0.05603638291358948\n",
      "Epoch 1: 0.025526676326990128\n",
      "Epoch 1: 0.07871516793966293\n"
     ]
    }
   ],
   "source": [
    "# train on task 2\n",
    "for e in range(n_epoch):\n",
    "    n_batch = floor(task2_tr_samples.shape[0] / minibatch_sz)\n",
    "    \n",
    "    for b in range(n_batch):\n",
    "        x_batch = train_data.data[task2_tr_samples[(b*minibatch_sz):((b+1)*minibatch_sz)]]\n",
    "        y_batch = train_data.targets[task2_tr_samples[(b*minibatch_sz):((b+1)*minibatch_sz)]]\n",
    "\n",
    "        # flatten image before presenting to the network and normalize intensities to the range [0, 1]\n",
    "        x_batch = torch.flatten(x_batch / 255, start_dim=1)\n",
    "\n",
    "        # convert label to one hot\n",
    "        y_batch = y_batch % 2\n",
    "        y_batch = F.one_hot(y_batch).float()\n",
    "\n",
    "        y_hat_batch = network(x_batch)\n",
    "        loss = criterion(y_hat_batch, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f'Epoch {e}: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 89.17257683215131\n"
     ]
    }
   ],
   "source": [
    "# test on Task 2\n",
    "n_batch = floor(task2_ts_samples.shape[0] / minibatch_sz)\n",
    "\n",
    "n_correct = 0    \n",
    "for b in range(n_batch):\n",
    "    x_batch = test_data.data[task2_ts_samples[(b*minibatch_sz):((b+1)*minibatch_sz)]]\n",
    "    y_batch = test_data.targets[task2_ts_samples[(b*minibatch_sz):((b+1)*minibatch_sz)]]\n",
    "\n",
    "    # flatten image before presenting to the network and normalize intensities to the range [0, 1]\n",
    "    x_batch = torch.flatten(x_batch / 255, start_dim=1)\n",
    "\n",
    "    y_hat_batch = network(x_batch)\n",
    "    _, prediction = torch.max(y_hat_batch, 1)\n",
    "    n_correct += (prediction == (y_batch % 2)).sum().item()\n",
    "\n",
    "print(f'Accuracy = {(n_correct * 100) / task1_ts_samples.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 57.4468085106383\n"
     ]
    }
   ],
   "source": [
    "# test on Task 1\n",
    "n_batch = floor(task1_ts_samples.shape[0] / minibatch_sz)\n",
    "\n",
    "n_correct = 0    \n",
    "for b in range(n_batch):\n",
    "    x_batch = test_data.data[task1_ts_samples[(b*minibatch_sz):((b+1)*minibatch_sz)]]\n",
    "    y_batch = test_data.targets[task1_ts_samples[(b*minibatch_sz):((b+1)*minibatch_sz)]]\n",
    "\n",
    "    # flatten image before presenting to the network and normalize intensities to the range [0, 1]\n",
    "    x_batch = torch.flatten(x_batch / 255, start_dim=1)\n",
    "\n",
    "    y_hat_batch = network(x_batch)\n",
    "    _, prediction = torch.max(y_hat_batch, 1)\n",
    "    n_correct += (prediction == y_batch).sum().item()\n",
    "\n",
    "print(f'Accuracy = {(n_correct * 100) / task1_ts_samples.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Question 2**: The purpose of this question is to study the effect of replay on catatophic forgetting. In this question also, we will train the network on two tasks in a sequence? When we train the network on the second task, we will also use some samples from the first task for replay. TO keep things simple, select a random proportaion (say 50%) of samples from the first task for replay. After training evaluate the performance of the trained network on both tasks. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save some samples from previous tasks for replay\n",
    "prop_saved = 0.5 # proportion of samples saved from a task for replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new network\n",
    "network = Network(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 0.6861187219619751\n",
      "Epoch 0: 0.6880649328231812\n",
      "Epoch 0: 0.6798537969589233\n",
      "Epoch 0: 0.6838418841362\n",
      "Epoch 0: 0.6823480129241943\n",
      "Epoch 0: 0.678693413734436\n",
      "Epoch 0: 0.6775755286216736\n",
      "Epoch 0: 0.678235650062561\n",
      "Epoch 0: 0.677001416683197\n",
      "Epoch 0: 0.6789224147796631\n",
      "Epoch 0: 0.6728288531303406\n",
      "Epoch 0: 0.667654275894165\n",
      "Epoch 0: 0.6719087362289429\n",
      "Epoch 0: 0.6661622524261475\n",
      "Epoch 0: 0.6608288288116455\n",
      "Epoch 0: 0.6627450585365295\n",
      "Epoch 0: 0.6563937067985535\n",
      "Epoch 0: 0.6439087390899658\n",
      "Epoch 0: 0.6464380025863647\n",
      "Epoch 0: 0.6473026275634766\n",
      "Epoch 0: 0.6406174898147583\n",
      "Epoch 0: 0.6351507902145386\n",
      "Epoch 0: 0.6325798034667969\n",
      "Epoch 0: 0.6348806619644165\n",
      "Epoch 0: 0.6348358392715454\n",
      "Epoch 0: 0.628645122051239\n",
      "Epoch 0: 0.5977924466133118\n",
      "Epoch 0: 0.601351261138916\n",
      "Epoch 0: 0.6065499782562256\n",
      "Epoch 0: 0.5875546336174011\n",
      "Epoch 0: 0.5756452083587646\n",
      "Epoch 0: 0.5907731056213379\n",
      "Epoch 0: 0.5582550168037415\n",
      "Epoch 0: 0.5601009726524353\n",
      "Epoch 0: 0.5380309820175171\n",
      "Epoch 0: 0.5446566939353943\n",
      "Epoch 0: 0.529500424861908\n",
      "Epoch 0: 0.5377429723739624\n",
      "Epoch 0: 0.5208322405815125\n",
      "Epoch 0: 0.4904223680496216\n",
      "Epoch 0: 0.5124381184577942\n",
      "Epoch 0: 0.5052633881568909\n",
      "Epoch 0: 0.47320592403411865\n",
      "Epoch 0: 0.452881783246994\n",
      "Epoch 0: 0.45034828782081604\n",
      "Epoch 0: 0.4340108335018158\n",
      "Epoch 0: 0.43824097514152527\n",
      "Epoch 0: 0.3934083580970764\n",
      "Epoch 0: 0.42965802550315857\n",
      "Epoch 0: 0.39997297525405884\n",
      "Epoch 0: 0.39725321531295776\n",
      "Epoch 0: 0.3604204058647156\n",
      "Epoch 0: 0.3561841547489166\n",
      "Epoch 0: 0.325335294008255\n",
      "Epoch 0: 0.31601616740226746\n",
      "Epoch 0: 0.30361855030059814\n",
      "Epoch 0: 0.3107071816921234\n",
      "Epoch 0: 0.2891559600830078\n",
      "Epoch 0: 0.32453402876853943\n",
      "Epoch 0: 0.2634517252445221\n",
      "Epoch 0: 0.214186429977417\n",
      "Epoch 0: 0.19839438796043396\n",
      "Epoch 0: 0.1955907791852951\n",
      "Epoch 0: 0.2053941786289215\n",
      "Epoch 0: 0.14161449670791626\n",
      "Epoch 0: 0.12478220462799072\n",
      "Epoch 0: 0.15894827246665955\n",
      "Epoch 0: 0.1570461392402649\n",
      "Epoch 0: 0.10553605109453201\n",
      "Epoch 0: 0.09227082133293152\n",
      "Epoch 0: 0.12131901830434799\n",
      "Epoch 0: 0.09516435861587524\n",
      "Epoch 0: 0.08875270187854767\n",
      "Epoch 0: 0.09277737885713577\n",
      "Epoch 0: 0.05348600447177887\n",
      "Epoch 0: 0.05094548314809799\n",
      "Epoch 0: 0.048488374799489975\n",
      "Epoch 0: 0.04326154664158821\n",
      "Epoch 0: 0.04506368562579155\n",
      "Epoch 0: 0.03723318874835968\n",
      "Epoch 0: 0.02768683061003685\n",
      "Epoch 0: 0.03458801284432411\n",
      "Epoch 0: 0.018900228664278984\n",
      "Epoch 0: 0.04605475068092346\n",
      "Epoch 0: 0.010258673690259457\n",
      "Epoch 0: 0.031080834567546844\n",
      "Epoch 0: 0.013284614309668541\n",
      "Epoch 0: 0.07031083852052689\n",
      "Epoch 0: 0.007688301149755716\n",
      "Epoch 0: 0.0037447591312229633\n",
      "Epoch 0: 0.006395046599209309\n",
      "Epoch 0: 0.005952131934463978\n",
      "Epoch 0: 0.051046665757894516\n",
      "Epoch 0: 0.0026282519102096558\n",
      "Epoch 0: 0.13175109028816223\n",
      "Epoch 0: 0.002476140158250928\n",
      "Epoch 0: 0.002357013989239931\n",
      "Epoch 0: 0.045754700899124146\n",
      "Epoch 0: 0.012111612595617771\n",
      "Epoch 0: 0.0037376820109784603\n",
      "Epoch 0: 0.06890121102333069\n",
      "Epoch 0: 0.0007645352743566036\n",
      "Epoch 0: 0.003921589348465204\n",
      "Epoch 0: 0.05985771119594574\n",
      "Epoch 0: 0.04755274951457977\n",
      "Epoch 0: 0.0054419622756540775\n",
      "Epoch 0: 0.0006967878434807062\n",
      "Epoch 0: 0.00044995927601121366\n",
      "Epoch 0: 0.0024683510418981314\n",
      "Epoch 0: 0.0006828364566899836\n",
      "Epoch 0: 0.0003388076729606837\n",
      "Epoch 0: 0.007561000995337963\n",
      "Epoch 0: 0.004040352068841457\n",
      "Epoch 0: 0.004451872780919075\n",
      "Epoch 0: 0.0016179694794118404\n",
      "Epoch 0: 0.021761344745755196\n",
      "Epoch 0: 3.137312160106376e-05\n",
      "Epoch 0: 1.272434110433096e-05\n",
      "Epoch 0: 0.00030080717988312244\n",
      "Epoch 0: 0.006488662678748369\n",
      "Epoch 0: 4.3916061258642e-06\n",
      "Epoch 0: 0.00027767603751271963\n",
      "Epoch 0: 0.0004014478763565421\n",
      "Epoch 0: 0.00015197592438198626\n",
      "Epoch 0: 0.0001750537776388228\n",
      "Epoch 0: 0.10088536888360977\n",
      "Epoch 0: 4.656227974919602e-05\n",
      "Epoch 0: 0.27031782269477844\n",
      "Epoch 0: 1.3727486702919123e-06\n",
      "Epoch 0: 1.3447843230096623e-06\n",
      "Epoch 0: 0.006008442956954241\n",
      "Epoch 0: 0.22800412774085999\n",
      "Epoch 0: 7.59947567985364e-07\n",
      "Epoch 0: 0.22210757434368134\n",
      "Epoch 0: 0.14250095188617706\n",
      "Epoch 0: 6.575004363185144e-07\n",
      "Epoch 0: 4.972605893271975e-06\n",
      "Epoch 0: 0.005998819600790739\n",
      "Epoch 0: 3.060495509998873e-05\n",
      "Epoch 0: 2.4214381610931923e-08\n",
      "Epoch 0: 0.12586265802383423\n",
      "Epoch 0: 4.656608609820978e-08\n",
      "Epoch 0: 3.7844392863917165e-06\n",
      "Epoch 0: 9.313223081619526e-09\n",
      "Epoch 0: -0.0\n",
      "Epoch 0: 0.11259137094020844\n",
      "Epoch 0: 5.9604559510262334e-08\n",
      "Epoch 0: 1.8626446163239052e-08\n",
      "Epoch 0: -0.0\n",
      "Epoch 0: -0.0\n",
      "Epoch 0: 0.049973778426647186\n",
      "Epoch 0: -0.0\n",
      "Epoch 0: 0.00010811309039127082\n",
      "Epoch 0: -0.0\n",
      "Epoch 0: 0.00025921568158082664\n",
      "Epoch 0: 4.6937952902226243e-07\n",
      "Epoch 0: 2.4214367400077208e-08\n",
      "Epoch 0: 7.078035224594714e-08\n",
      "Epoch 0: -0.0\n",
      "Epoch 0: 5.587934559514451e-09\n",
      "Epoch 0: -0.0\n",
      "Epoch 0: -0.0\n",
      "Epoch 0: 0.02399454638361931\n",
      "Epoch 0: -0.0\n",
      "Epoch 0: 2.235172757991677e-08\n",
      "Epoch 0: -0.0\n",
      "Epoch 0: 0.3028746247291565\n",
      "Epoch 0: 0.5381157398223877\n",
      "Epoch 0: -0.0\n",
      "Epoch 0: -0.0\n",
      "Epoch 0: 1.0223070603387896e-05\n",
      "Epoch 0: -0.0\n",
      "Epoch 0: 3.7252898543727042e-09\n",
      "Epoch 0: 0.08984875679016113\n",
      "Epoch 0: 5.121435151522746e-06\n",
      "Epoch 0: -0.0\n",
      "Epoch 0: -0.0\n",
      "Epoch 0: -0.0\n",
      "Epoch 0: -0.0\n",
      "Epoch 0: 1.4901154088420299e-08\n",
      "Epoch 0: -0.0\n",
      "Epoch 0: 1.8626450382086546e-09\n",
      "Epoch 0: -0.0\n",
      "Epoch 0: -0.0\n",
      "Epoch 0: 5.4729134717490524e-05\n",
      "Epoch 0: 1.8626450382086546e-09\n",
      "Epoch 0: -0.0\n",
      "Epoch 0: -0.0\n",
      "Epoch 0: -0.0\n",
      "Epoch 0: -0.0\n",
      "Epoch 0: -0.0\n",
      "Epoch 0: -0.0\n",
      "Epoch 0: -0.0\n",
      "Epoch 0: 0.006417383905500174\n",
      "Epoch 0: -0.0\n",
      "Epoch 0: -0.0\n",
      "Epoch 0: -0.0\n",
      "Epoch 1: 0.6321088075637817\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: 0.594957709312439\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: 1.8626450382086546e-09\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: 0.9622800350189209\n",
      "Epoch 1: 0.5435580015182495\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: 0.00034424150362610817\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: 1.3177930116653442\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: 0.3207543194293976\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: 1.8626450382086546e-09\n",
      "Epoch 1: 0.06829103082418442\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: 0.8807792067527771\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: 0.6053829193115234\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: 1.8626450382086546e-09\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: 0.13164976239204407\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: 0.027578849345445633\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: 7.450578820566989e-09\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: 2.351147413253784\n",
      "Epoch 1: 0.8927597999572754\n",
      "Epoch 1: 1.2819716930389404\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: 1.102508544921875\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: 1.434230227914668e-07\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: 0.40901944041252136\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: 0.8589200973510742\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: 3.625633716583252\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: 5.587934559514451e-09\n",
      "Epoch 1: 0.3150678873062134\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: 0.7441047430038452\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: 0.6347267627716064\n",
      "Epoch 1: 0.7241533994674683\n",
      "Epoch 1: 8.259451715275645e-05\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: 0.849385142326355\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: 1.8626450382086546e-09\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: 3.0677826404571533\n",
      "Epoch 1: 0.028537560254335403\n",
      "Epoch 1: 0.09228210151195526\n",
      "Epoch 1: 0.006639283616095781\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: 0.16052885353565216\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: 0.29869401454925537\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: 1.2272660732269287\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: 1.5887554809523863e-06\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: 1.2605020999908447\n",
      "Epoch 1: 4.315921306610107\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: -0.0\n"
     ]
    }
   ],
   "source": [
    "# train on task 1\n",
    "for e in range(n_epoch):\n",
    "    n_batch = floor(task1_tr_samples.shape[0] / minibatch_sz)\n",
    "    \n",
    "    for b in range(n_batch):\n",
    "        x_batch = train_data.data[task1_tr_samples[(b*minibatch_sz):((b+1)*minibatch_sz)]]\n",
    "        y_batch = train_data.targets[task1_tr_samples[(b*minibatch_sz):((b+1)*minibatch_sz)]]\n",
    "\n",
    "        # flatten image before presenting to the network and normalize intensities to the range [0, 1]\n",
    "        x_batch = torch.flatten(x_batch / 255, start_dim=1)\n",
    "\n",
    "        # convert label to one hot\n",
    "        y_batch = F.one_hot(y_batch).float()\n",
    "\n",
    "        y_hat_batch = network(x_batch)\n",
    "        loss = criterion(y_hat_batch, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f'Epoch {e}: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "task1_replay = np.random.choice(task1_tr_samples.numpy(), int(prop_saved * task1_tr_samples.shape[0]))\n",
    "task1_replay_samples = torch.Tensor(task1_replay).int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 74.47444152832031\n",
      "Epoch 0: 85.4444351196289\n",
      "Epoch 0: 52.0087890625\n",
      "Epoch 0: 56.47416305541992\n",
      "Epoch 0: 58.91046142578125\n",
      "Epoch 0: 20.745325088500977\n",
      "Epoch 0: 61.7367057800293\n",
      "Epoch 0: 59.45780944824219\n",
      "Epoch 0: 35.815460205078125\n",
      "Epoch 0: 31.981496810913086\n",
      "Epoch 0: 10.560304641723633\n",
      "Epoch 0: 18.08081817626953\n",
      "Epoch 0: 23.73309326171875\n",
      "Epoch 0: 9.656204223632812\n",
      "Epoch 0: 10.612948417663574\n",
      "Epoch 0: 13.979422569274902\n",
      "Epoch 0: 17.6220703125\n",
      "Epoch 0: 33.5146484375\n",
      "Epoch 0: 30.247905731201172\n",
      "Epoch 0: 19.095428466796875\n",
      "Epoch 0: 9.698869705200195\n",
      "Epoch 0: 7.056517601013184\n",
      "Epoch 0: 1.79326331615448\n",
      "Epoch 0: 8.665701866149902\n",
      "Epoch 0: 11.909594535827637\n",
      "Epoch 0: 29.386981964111328\n",
      "Epoch 0: 34.13988494873047\n",
      "Epoch 0: 35.49656677246094\n",
      "Epoch 0: 23.07880401611328\n",
      "Epoch 0: 18.847185134887695\n",
      "Epoch 0: 6.588086128234863\n",
      "Epoch 0: 6.474761962890625\n",
      "Epoch 0: 4.687563419342041\n",
      "Epoch 0: 8.245673179626465\n",
      "Epoch 0: 8.287185668945312\n",
      "Epoch 0: 15.62210750579834\n",
      "Epoch 0: 16.157024383544922\n",
      "Epoch 0: 10.239376068115234\n",
      "Epoch 0: 10.960902214050293\n",
      "Epoch 0: 8.565625190734863\n",
      "Epoch 0: 9.852270126342773\n",
      "Epoch 0: 8.171308517456055\n",
      "Epoch 0: 5.185338020324707\n",
      "Epoch 0: 3.405477523803711\n",
      "Epoch 0: 6.479763984680176\n",
      "Epoch 0: 2.415916919708252\n",
      "Epoch 0: 1.8010598421096802\n",
      "Epoch 0: 0.11717420816421509\n",
      "Epoch 0: 1.2367794513702393\n",
      "Epoch 0: 0.32340168952941895\n",
      "Epoch 0: 0.1967128962278366\n",
      "Epoch 0: 0.6734287738800049\n",
      "Epoch 0: 0.27216872572898865\n",
      "Epoch 0: 0.39551031589508057\n",
      "Epoch 0: 1.0262956619262695\n",
      "Epoch 0: 0.8670606017112732\n",
      "Epoch 0: 0.7372428774833679\n",
      "Epoch 0: 1.3484302759170532\n",
      "Epoch 0: 0.41515982151031494\n",
      "Epoch 0: 0.47005945444107056\n",
      "Epoch 0: 0.5181999206542969\n",
      "Epoch 0: 0.44953227043151855\n",
      "Epoch 0: 0.6712690591812134\n",
      "Epoch 0: 0.5007398724555969\n",
      "Epoch 0: 0.9564185738563538\n",
      "Epoch 0: 0.6267557144165039\n",
      "Epoch 0: 1.183705449104309\n",
      "Epoch 0: 1.284217357635498\n",
      "Epoch 0: 2.047269582748413\n",
      "Epoch 0: 2.521667242050171\n",
      "Epoch 0: 1.6598646640777588\n",
      "Epoch 0: 2.7839982509613037\n",
      "Epoch 0: 1.437347173690796\n",
      "Epoch 0: 2.7984158992767334\n",
      "Epoch 0: 2.292508602142334\n",
      "Epoch 0: 1.5649510622024536\n",
      "Epoch 0: 0.9982275366783142\n",
      "Epoch 0: 0.8715546131134033\n",
      "Epoch 0: 1.8153712749481201\n",
      "Epoch 0: 0.9303542375564575\n",
      "Epoch 0: 1.0506738424301147\n",
      "Epoch 0: 0.21486696600914001\n",
      "Epoch 0: 0.5269653797149658\n",
      "Epoch 0: 0.7922853827476501\n",
      "Epoch 0: 0.46210911870002747\n",
      "Epoch 0: 0.10971075296401978\n",
      "Epoch 0: 1.9569844007492065\n",
      "Epoch 0: 0.6022793054580688\n",
      "Epoch 0: 0.18972355127334595\n",
      "Epoch 0: 0.6826996803283691\n",
      "Epoch 0: 1.0853893756866455\n",
      "Epoch 0: 1.5191411972045898\n",
      "Epoch 0: 1.6717555522918701\n",
      "Epoch 0: 1.4756731986999512\n",
      "Epoch 0: 0.6774563789367676\n",
      "Epoch 0: 0.36673277616500854\n",
      "Epoch 0: 0.3869815766811371\n",
      "Epoch 0: 0.34491387009620667\n",
      "Epoch 0: 0.3740224838256836\n",
      "Epoch 0: 0.30313387513160706\n",
      "Epoch 0: 0.252781480550766\n",
      "Epoch 0: 0.21542128920555115\n",
      "Epoch 0: 0.2893321216106415\n",
      "Epoch 0: 0.1278674602508545\n",
      "Epoch 0: 0.4336920380592346\n",
      "Epoch 0: 0.4362892508506775\n",
      "Epoch 0: 0.8932472467422485\n",
      "Epoch 0: 1.7118051052093506\n",
      "Epoch 0: 1.5343257188796997\n",
      "Epoch 0: 5.758247375488281\n",
      "Epoch 0: 0.09271715581417084\n",
      "Epoch 0: 0.9963428974151611\n",
      "Epoch 0: 0.59565669298172\n",
      "Epoch 0: 0.3006821572780609\n",
      "Epoch 0: 0.4729495942592621\n",
      "Epoch 0: 2.9296774864196777\n",
      "Epoch 0: 0.5624107718467712\n",
      "Epoch 0: 0.2974669337272644\n",
      "Epoch 0: 0.5080658197402954\n",
      "Epoch 0: 0.9175359606742859\n",
      "Epoch 0: 0.8593653440475464\n",
      "Epoch 0: 0.21991977095603943\n",
      "Epoch 0: 0.487215518951416\n",
      "Epoch 0: 0.9530978798866272\n",
      "Epoch 0: 3.640212297439575\n",
      "Epoch 0: 0.6437452435493469\n",
      "Epoch 0: 0.23739443719387054\n",
      "Epoch 0: 0.024711646139621735\n",
      "Epoch 0: 0.2149670124053955\n",
      "Epoch 0: 0.1925405114889145\n",
      "Epoch 0: 0.5668031573295593\n",
      "Epoch 0: 0.2292875498533249\n",
      "Epoch 0: 0.16786794364452362\n",
      "Epoch 0: 0.49991366267204285\n",
      "Epoch 0: 0.8129326105117798\n",
      "Epoch 0: 0.6221802234649658\n",
      "Epoch 0: 0.8708535432815552\n",
      "Epoch 0: 0.1296665519475937\n",
      "Epoch 0: 0.6082828640937805\n",
      "Epoch 0: 0.08787347376346588\n",
      "Epoch 0: 0.11296650022268295\n",
      "Epoch 0: 6.953115463256836\n",
      "Epoch 0: 1.6326484680175781\n",
      "Epoch 0: 0.4332444667816162\n",
      "Epoch 0: 1.3062808513641357\n",
      "Epoch 0: 4.278081893920898\n",
      "Epoch 0: 0.28945809602737427\n",
      "Epoch 0: 0.26847484707832336\n",
      "Epoch 0: 6.363557815551758\n",
      "Epoch 0: 0.3773307204246521\n",
      "Epoch 0: 1.6142022609710693\n",
      "Epoch 0: 0.44848310947418213\n",
      "Epoch 0: 0.06431201845407486\n",
      "Epoch 0: 0.39562031626701355\n",
      "Epoch 0: 0.9273375868797302\n",
      "Epoch 0: 0.02165215089917183\n",
      "Epoch 0: 0.6222736239433289\n",
      "Epoch 0: 1.106907844543457\n",
      "Epoch 0: 0.9456740617752075\n",
      "Epoch 0: 2.289858818054199\n",
      "Epoch 0: 0.29598310589790344\n",
      "Epoch 0: 1.2684249877929688\n",
      "Epoch 0: 0.6367203593254089\n",
      "Epoch 0: 1.1566359996795654\n",
      "Epoch 0: 9.56379222869873\n",
      "Epoch 0: 0.4840032458305359\n",
      "Epoch 0: 0.3405100107192993\n",
      "Epoch 0: 1.3744828701019287\n",
      "Epoch 0: 3.2626688480377197\n",
      "Epoch 0: 0.7454866170883179\n",
      "Epoch 0: 3.445955276489258\n",
      "Epoch 0: 1.2987779378890991\n",
      "Epoch 0: 0.4185614287853241\n",
      "Epoch 0: 0.011615701019763947\n",
      "Epoch 0: 0.9453948736190796\n",
      "Epoch 0: 0.06947002559900284\n",
      "Epoch 0: 0.24710822105407715\n",
      "Epoch 0: 0.23552533984184265\n",
      "Epoch 0: 0.5335274338722229\n",
      "Epoch 0: 0.3432956337928772\n",
      "Epoch 0: 1.277742862701416\n",
      "Epoch 0: 0.9594843983650208\n",
      "Epoch 0: 0.36245232820510864\n",
      "Epoch 0: 0.20731014013290405\n",
      "Epoch 0: 0.6583183407783508\n",
      "Epoch 0: 0.32631704211235046\n",
      "Epoch 0: 1.0622600317001343\n",
      "Epoch 0: 0.011317354626953602\n",
      "Epoch 0: 0.6409246325492859\n",
      "Epoch 0: 1.8511165380477905\n",
      "Epoch 0: 3.1381144523620605\n",
      "Epoch 0: 2.0273594856262207\n",
      "Epoch 0: 1.3289204835891724\n",
      "Epoch 0: 0.7168788909912109\n",
      "Epoch 0: 1.956277847290039\n",
      "Epoch 0: 0.237511545419693\n",
      "Epoch 0: 3.2514283657073975\n",
      "Epoch 0: 1.182950496673584\n",
      "Epoch 0: 3.2849478721618652\n",
      "Epoch 0: 0.42355188727378845\n",
      "Epoch 0: 1.7431349754333496\n",
      "Epoch 0: 1.2805451154708862\n",
      "Epoch 0: 1.0138930082321167\n",
      "Epoch 0: 1.3383307456970215\n",
      "Epoch 0: 1.991070032119751\n",
      "Epoch 0: 1.3860257863998413\n",
      "Epoch 0: 1.5970488786697388\n",
      "Epoch 0: 1.9065873622894287\n",
      "Epoch 0: 3.407160997390747\n",
      "Epoch 0: 1.9526124000549316\n",
      "Epoch 0: 1.489389419555664\n",
      "Epoch 0: 1.0119940042495728\n",
      "Epoch 0: 0.6878232955932617\n",
      "Epoch 0: 0.8778152465820312\n",
      "Epoch 0: 9.905638580676168e-05\n",
      "Epoch 0: 0.3011040985584259\n",
      "Epoch 0: 1.7238414287567139\n",
      "Epoch 0: 1.443303108215332\n",
      "Epoch 0: 1.9609415531158447\n",
      "Epoch 0: 1.7474994659423828\n",
      "Epoch 0: 0.9225257635116577\n",
      "Epoch 0: 6.133484840393066\n",
      "Epoch 0: 0.13991732895374298\n",
      "Epoch 0: 0.24887454509735107\n",
      "Epoch 0: 0.5476192235946655\n",
      "Epoch 0: 0.29284292459487915\n",
      "Epoch 0: 0.3305501937866211\n",
      "Epoch 0: 0.0006857867119833827\n",
      "Epoch 0: 0.9623209238052368\n",
      "Epoch 0: 0.8073261976242065\n",
      "Epoch 0: 1.5347245931625366\n",
      "Epoch 0: 1.3289117813110352\n",
      "Epoch 0: 1.58262300491333\n",
      "Epoch 0: 1.3129408359527588\n",
      "Epoch 0: 0.30436569452285767\n",
      "Epoch 0: 0.0717615857720375\n",
      "Epoch 0: 5.138218057254562e-06\n",
      "Epoch 0: 0.609063982963562\n",
      "Epoch 0: 0.9416730999946594\n",
      "Epoch 0: 0.20365317165851593\n",
      "Epoch 0: 0.49180421233177185\n",
      "Epoch 0: 0.4841621518135071\n",
      "Epoch 0: 3.0303595066070557\n",
      "Epoch 0: 1.7024394273757935\n",
      "Epoch 0: 0.7330613732337952\n",
      "Epoch 0: 0.9066914319992065\n",
      "Epoch 0: 1.8644059896469116\n",
      "Epoch 0: 4.145883083343506\n",
      "Epoch 0: 0.21106532216072083\n",
      "Epoch 0: 1.7637362480163574\n",
      "Epoch 0: 3.0585734844207764\n",
      "Epoch 0: 3.9115059280447895e-07\n",
      "Epoch 0: 1.151049256324768\n",
      "Epoch 0: 4.935931769978197e-07\n",
      "Epoch 0: 3.5033464431762695\n",
      "Epoch 0: 0.20522719621658325\n",
      "Epoch 0: 0.3223763704299927\n",
      "Epoch 0: 0.000444867619080469\n",
      "Epoch 0: 3.744623899459839\n",
      "Epoch 0: 4.582107067108154\n",
      "Epoch 0: 0.023773804306983948\n",
      "Epoch 0: 0.0021153376437723637\n",
      "Epoch 0: 4.209521193843102e-07\n",
      "Epoch 0: 1.2741718292236328\n",
      "Epoch 0: 0.040192414075136185\n",
      "Epoch 0: -0.0\n",
      "Epoch 0: 1.1014854907989502\n",
      "Epoch 0: 0.2102264016866684\n",
      "Epoch 0: 3.3201045989990234\n",
      "Epoch 0: 3.2838222980499268\n",
      "Epoch 0: 6.53573751449585\n",
      "Epoch 0: 2.3257410526275635\n",
      "Epoch 0: 0.7728818655014038\n",
      "Epoch 0: 3.326011896133423\n",
      "Epoch 0: 1.142970085144043\n",
      "Epoch 0: 3.1589746475219727\n",
      "Epoch 0: 0.5223025679588318\n",
      "Epoch 0: 6.549227237701416\n",
      "Epoch 0: 2.1981985569000244\n",
      "Epoch 0: 3.7355399131774902\n",
      "Epoch 0: 1.8722033500671387\n",
      "Epoch 0: 2.336467742919922\n",
      "Epoch 0: 3.2942299842834473\n",
      "Epoch 0: 6.695512771606445\n",
      "Epoch 0: 10.641985893249512\n",
      "Epoch 0: 2.3118441104888916\n",
      "Epoch 0: 3.4606356620788574\n",
      "Epoch 1: 6.772359371185303\n",
      "Epoch 1: 4.281206130981445\n",
      "Epoch 1: 3.502401351928711\n",
      "Epoch 1: 1.9242911338806152\n",
      "Epoch 1: 4.454480171203613\n",
      "Epoch 1: 0.0006436291732825339\n",
      "Epoch 1: 0.7945833802223206\n",
      "Epoch 1: 0.5925050377845764\n",
      "Epoch 1: 0.0384485088288784\n",
      "Epoch 1: 2.124460220336914\n",
      "Epoch 1: 5.151212692260742\n",
      "Epoch 1: 7.624147415161133\n",
      "Epoch 1: 7.958575248718262\n",
      "Epoch 1: 11.99096965789795\n",
      "Epoch 1: 9.605390548706055\n",
      "Epoch 1: 0.9362162351608276\n",
      "Epoch 1: 1.334673285484314\n",
      "Epoch 1: 5.253414154052734\n",
      "Epoch 1: 1.1133619546890259\n",
      "Epoch 1: 3.0372490882873535\n",
      "Epoch 1: 2.5680949687957764\n",
      "Epoch 1: 0.7326614260673523\n",
      "Epoch 1: 1.6011550426483154\n",
      "Epoch 1: 3.089515447616577\n",
      "Epoch 1: 0.041017286479473114\n",
      "Epoch 1: 14.171741485595703\n",
      "Epoch 1: 0.561470091342926\n",
      "Epoch 1: 0.10027824342250824\n",
      "Epoch 1: 2.4616308212280273\n",
      "Epoch 1: 1.8726555109024048\n",
      "Epoch 1: 1.2221795320510864\n",
      "Epoch 1: 1.049590826034546\n",
      "Epoch 1: 1.0529706478118896\n",
      "Epoch 1: 7.188027858734131\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: 2.9441404342651367\n",
      "Epoch 1: 1.5483365058898926\n",
      "Epoch 1: 1.2437610626220703\n",
      "Epoch 1: 1.8695595264434814\n",
      "Epoch 1: 2.05698561668396\n",
      "Epoch 1: 4.47851037979126\n",
      "Epoch 1: 7.249470233917236\n",
      "Epoch 1: 0.6298016309738159\n",
      "Epoch 1: 1.1255823373794556\n",
      "Epoch 1: 1.8488638401031494\n",
      "Epoch 1: 2.9146218299865723\n",
      "Epoch 1: 3.9981000423431396\n",
      "Epoch 1: 0.5034882426261902\n",
      "Epoch 1: 2.262883186340332\n",
      "Epoch 1: 1.8925844430923462\n",
      "Epoch 1: 0.7803341150283813\n",
      "Epoch 1: 1.0748271942138672\n",
      "Epoch 1: 2.4206037521362305\n",
      "Epoch 1: 4.665979385375977\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: 2.2586772441864014\n",
      "Epoch 1: 0.4637807607650757\n",
      "Epoch 1: 0.9406358003616333\n",
      "Epoch 1: 0.708647608757019\n",
      "Epoch 1: 0.09861909598112106\n",
      "Epoch 1: 2.6701784133911133\n",
      "Epoch 1: -0.0\n",
      "Epoch 1: 1.27110755443573\n",
      "Epoch 1: 0.31125062704086304\n",
      "Epoch 1: 0.9265176057815552\n",
      "Epoch 1: 2.357008695602417\n",
      "Epoch 1: 2.5039877891540527\n",
      "Epoch 1: 3.2829298973083496\n",
      "Epoch 1: 1.4758914709091187\n",
      "Epoch 1: 0.4526973068714142\n",
      "Epoch 1: 0.8101658225059509\n",
      "Epoch 1: 3.3972558975219727\n",
      "Epoch 1: 2.197912607471153e-07\n",
      "Epoch 1: 0.19643837213516235\n",
      "Epoch 1: 0.7335634231567383\n",
      "Epoch 1: 3.7540342807769775\n",
      "Epoch 1: 1.9121490716934204\n",
      "Epoch 1: 0.9340694546699524\n",
      "Epoch 1: 0.28740012645721436\n",
      "Epoch 1: 0.8094999194145203\n",
      "Epoch 1: 2.3096674794942373e-07\n",
      "Epoch 1: 0.008336104452610016\n",
      "Epoch 1: 0.057005345821380615\n",
      "Epoch 1: 0.07045578956604004\n",
      "Epoch 1: 0.40380972623825073\n",
      "Epoch 1: 0.037299856543540955\n",
      "Epoch 1: 2.2548375129699707\n",
      "Epoch 1: 0.3088948428630829\n",
      "Epoch 1: 2.1371231079101562\n",
      "Epoch 1: 1.805094599723816\n",
      "Epoch 1: 1.8443388938903809\n",
      "Epoch 1: 1.3190011978149414\n",
      "Epoch 1: 0.6982612609863281\n",
      "Epoch 1: 0.6751150488853455\n",
      "Epoch 1: 0.08130773156881332\n",
      "Epoch 1: 0.08404693007469177\n",
      "Epoch 1: 0.01748286932706833\n",
      "Epoch 1: 6.537749754897959e-07\n",
      "Epoch 1: 0.004279064014554024\n",
      "Epoch 1: 0.06326238811016083\n",
      "Epoch 1: 0.14271782338619232\n",
      "Epoch 1: 4.1188868635799736e-05\n",
      "Epoch 1: 0.46268293261528015\n",
      "Epoch 1: 0.048742182552814484\n",
      "Epoch 1: 0.3303965628147125\n",
      "Epoch 1: 0.23498980700969696\n",
      "Epoch 1: 0.5578862428665161\n",
      "Epoch 1: 0.19874906539916992\n",
      "Epoch 1: 0.4696713089942932\n",
      "Epoch 1: 0.8616352677345276\n",
      "Epoch 1: 0.09963648021221161\n",
      "Epoch 1: 0.0031138998456299305\n",
      "Epoch 1: 0.0007355244597420096\n",
      "Epoch 1: 0.06724908947944641\n",
      "Epoch 1: 0.102448970079422\n",
      "Epoch 1: 0.1900276094675064\n",
      "Epoch 1: 0.05545834079384804\n",
      "Epoch 1: 0.10014011710882187\n",
      "Epoch 1: 0.694158136844635\n",
      "Epoch 1: 0.0409667082130909\n",
      "Epoch 1: 0.15770161151885986\n",
      "Epoch 1: 0.07713820785284042\n",
      "Epoch 1: 0.08808126300573349\n",
      "Epoch 1: 4.88311767578125\n",
      "Epoch 1: 1.6733956336975098\n",
      "Epoch 1: 0.8940895795822144\n",
      "Epoch 1: 0.05955808237195015\n",
      "Epoch 1: 0.127412348985672\n",
      "Epoch 1: 0.659620463848114\n",
      "Epoch 1: 0.12091552466154099\n",
      "Epoch 1: 0.23910170793533325\n",
      "Epoch 1: 0.043133072555065155\n",
      "Epoch 1: 0.5418197512626648\n",
      "Epoch 1: 0.18306921422481537\n",
      "Epoch 1: 0.13012105226516724\n",
      "Epoch 1: 0.09776671975851059\n",
      "Epoch 1: 0.2888558804988861\n",
      "Epoch 1: 0.007754344958811998\n",
      "Epoch 1: 0.10897183418273926\n",
      "Epoch 1: 0.09288563579320908\n",
      "Epoch 1: 0.027536839246749878\n",
      "Epoch 1: 0.6536837816238403\n",
      "Epoch 1: 0.48216742277145386\n",
      "Epoch 1: 0.024786662310361862\n",
      "Epoch 1: 1.1460264921188354\n",
      "Epoch 1: 0.16019709408283234\n",
      "Epoch 1: 0.378958523273468\n",
      "Epoch 1: 0.038770534098148346\n",
      "Epoch 1: 0.6029672622680664\n",
      "Epoch 1: 0.007290613371878862\n",
      "Epoch 1: 2.1432251930236816\n",
      "Epoch 1: 0.22714315354824066\n",
      "Epoch 1: 0.5063568949699402\n",
      "Epoch 1: 0.3025611937046051\n",
      "Epoch 1: 0.821928858757019\n",
      "Epoch 1: 0.05802107974886894\n",
      "Epoch 1: 0.09275446087121964\n",
      "Epoch 1: 0.2835339605808258\n",
      "Epoch 1: 0.12336033582687378\n",
      "Epoch 1: 0.9268811941146851\n",
      "Epoch 1: 0.0790727511048317\n",
      "Epoch 1: 0.18186131119728088\n",
      "Epoch 1: 3.11533260345459\n",
      "Epoch 1: 0.7987275123596191\n",
      "Epoch 1: 0.4529997706413269\n",
      "Epoch 1: 0.10407222807407379\n",
      "Epoch 1: 0.0036723618395626545\n",
      "Epoch 1: 0.3377974033355713\n",
      "Epoch 1: 0.033202171325683594\n",
      "Epoch 1: 0.04567456617951393\n",
      "Epoch 1: 0.08746787160634995\n",
      "Epoch 1: 0.4897986352443695\n",
      "Epoch 1: 0.24783097207546234\n",
      "Epoch 1: 0.06756524741649628\n",
      "Epoch 1: 0.03846265375614166\n",
      "Epoch 1: 0.06850321590900421\n",
      "Epoch 1: 0.15359342098236084\n",
      "Epoch 1: 0.08679234981536865\n",
      "Epoch 1: 0.31442147493362427\n",
      "Epoch 1: 0.03806081414222717\n",
      "Epoch 1: 0.03150143101811409\n",
      "Epoch 1: 0.6049601435661316\n",
      "Epoch 1: 0.009158743545413017\n",
      "Epoch 1: 0.026240196079015732\n",
      "Epoch 1: 0.11286556720733643\n",
      "Epoch 1: 0.04658083990216255\n",
      "Epoch 1: 0.01095274556428194\n",
      "Epoch 1: 0.07095310837030411\n",
      "Epoch 1: 0.15540005266666412\n",
      "Epoch 1: 0.13499434292316437\n",
      "Epoch 1: 0.561932384967804\n",
      "Epoch 1: 0.4098706841468811\n",
      "Epoch 1: 0.26471900939941406\n",
      "Epoch 1: 0.07214333862066269\n",
      "Epoch 1: 0.31993913650512695\n",
      "Epoch 1: 0.19486039876937866\n",
      "Epoch 1: 0.22908836603164673\n",
      "Epoch 1: 0.029081836342811584\n",
      "Epoch 1: 0.04044395312666893\n",
      "Epoch 1: 0.16245701909065247\n",
      "Epoch 1: 0.07211975753307343\n",
      "Epoch 1: 0.07778061926364899\n",
      "Epoch 1: 0.8550035357475281\n",
      "Epoch 1: 0.1334185004234314\n",
      "Epoch 1: 0.006690920330584049\n",
      "Epoch 1: 0.30596235394477844\n",
      "Epoch 1: 0.5172070264816284\n",
      "Epoch 1: 0.7023480534553528\n",
      "Epoch 1: 0.12633278965950012\n",
      "Epoch 1: 0.02949136681854725\n",
      "Epoch 1: 0.01733352243900299\n",
      "Epoch 1: 0.24848414957523346\n",
      "Epoch 1: 0.5241633653640747\n",
      "Epoch 1: 0.26387399435043335\n",
      "Epoch 1: 0.05097579583525658\n",
      "Epoch 1: 0.1563330441713333\n",
      "Epoch 1: 0.07176962494850159\n",
      "Epoch 1: 0.06614158302545547\n",
      "Epoch 1: 0.10092315822839737\n",
      "Epoch 1: 0.13725553452968597\n",
      "Epoch 1: 0.07018880546092987\n",
      "Epoch 1: 0.7349488735198975\n",
      "Epoch 1: 0.31557899713516235\n",
      "Epoch 1: 0.02172473445534706\n",
      "Epoch 1: 0.0540933683514595\n",
      "Epoch 1: 0.31568199396133423\n",
      "Epoch 1: 0.053717758506536484\n",
      "Epoch 1: 0.0038277648855000734\n",
      "Epoch 1: 0.3340633809566498\n",
      "Epoch 1: 0.28883808851242065\n",
      "Epoch 1: 0.3898518979549408\n",
      "Epoch 1: 0.09161638468503952\n",
      "Epoch 1: 1.424062967300415\n",
      "Epoch 1: 0.06344454735517502\n",
      "Epoch 1: 0.3386008143424988\n",
      "Epoch 1: 0.22232314944267273\n",
      "Epoch 1: 0.1010817289352417\n",
      "Epoch 1: 0.10228143632411957\n",
      "Epoch 1: 0.08598652482032776\n",
      "Epoch 1: 0.004970966372638941\n",
      "Epoch 1: 0.061524394899606705\n",
      "Epoch 1: 0.07312332838773727\n",
      "Epoch 1: 0.23997777700424194\n",
      "Epoch 1: 0.010304346680641174\n",
      "Epoch 1: 0.15990233421325684\n",
      "Epoch 1: 0.0681169331073761\n",
      "Epoch 1: 0.151704341173172\n",
      "Epoch 1: 0.15044090151786804\n",
      "Epoch 1: 0.011238164268434048\n",
      "Epoch 1: 0.14008380472660065\n",
      "Epoch 1: 0.05901632830500603\n",
      "Epoch 1: 0.14508163928985596\n",
      "Epoch 1: 0.0373811349272728\n",
      "Epoch 1: 0.05686354264616966\n",
      "Epoch 1: 0.643036961555481\n",
      "Epoch 1: 0.00024235491582658142\n",
      "Epoch 1: 0.08884784579277039\n",
      "Epoch 1: 0.02055952697992325\n",
      "Epoch 1: 0.04643973708152771\n",
      "Epoch 1: 0.38163721561431885\n",
      "Epoch 1: 0.0002846279530785978\n",
      "Epoch 1: 0.008514953777194023\n",
      "Epoch 1: 0.0010852727573364973\n",
      "Epoch 1: 0.15360230207443237\n",
      "Epoch 1: 0.10640139877796173\n",
      "Epoch 1: 0.00013564097753260285\n",
      "Epoch 1: 0.22385206818580627\n",
      "Epoch 1: 0.0025889056269079447\n",
      "Epoch 1: 0.09282306581735611\n",
      "Epoch 1: 0.0004638445971067995\n",
      "Epoch 1: 0.07389899343252182\n",
      "Epoch 1: 0.48582595586776733\n",
      "Epoch 1: 0.11639218777418137\n",
      "Epoch 1: 0.03455841541290283\n",
      "Epoch 1: 0.022318130359053612\n",
      "Epoch 1: 0.035676922649145126\n",
      "Epoch 1: 0.2752548158168793\n",
      "Epoch 1: 0.5665687918663025\n",
      "Epoch 1: 0.9736791849136353\n",
      "Epoch 1: 0.20270493626594543\n",
      "Epoch 1: 0.11822914332151413\n",
      "Epoch 1: 0.7359411716461182\n",
      "Epoch 1: 0.29231739044189453\n",
      "Epoch 1: 0.2435062974691391\n",
      "Epoch 1: 4.312029341235757e-05\n",
      "Epoch 1: 0.0021275340113788843\n",
      "Epoch 1: 8.921886660573364e-07\n"
     ]
    }
   ],
   "source": [
    "# train on task 2 with replay\n",
    "tr_samples = torch.concatenate([task2_tr_samples, task1_replay_samples], dim=0) # concatenate samples from task 2 and replay samples from task 1\n",
    "\n",
    "# randomize the array to mix samples from task 2 and replay\n",
    "np.random.shuffle(tr_samples.numpy())\n",
    "\n",
    "n_batch = floor(tr_samples.shape[0] / minibatch_sz)\n",
    "\n",
    "for e in range(n_epoch):    \n",
    "    for b in range(n_batch):\n",
    "        x_batch = train_data.data[tr_samples[(b*minibatch_sz):((b+1)*minibatch_sz)]]\n",
    "        y_batch = train_data.targets[tr_samples[(b*minibatch_sz):((b+1)*minibatch_sz)]]\n",
    "\n",
    "        # flatten image before presenting to the network and normalize intensities to the range [0, 1]\n",
    "        x_batch = torch.flatten(x_batch / 255, start_dim=1)\n",
    "\n",
    "        # convert label to one hot\n",
    "        y_batch = y_batch % 2\n",
    "        y_batch = F.one_hot(y_batch).float()\n",
    "\n",
    "        y_hat_batch = network(x_batch)\n",
    "        loss = criterion(y_hat_batch, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f'Epoch {e}: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 98.10874704491725\n"
     ]
    }
   ],
   "source": [
    "# test on Task 1\n",
    "n_batch = floor(task1_ts_samples.shape[0] / minibatch_sz)\n",
    "\n",
    "n_correct = 0    \n",
    "for b in range(n_batch):\n",
    "    x_batch = test_data.data[task1_ts_samples[(b*minibatch_sz):((b+1)*minibatch_sz)]]\n",
    "    y_batch = test_data.targets[task1_ts_samples[(b*minibatch_sz):((b+1)*minibatch_sz)]]\n",
    "\n",
    "    # flatten image before presenting to the network and normalize intensities to the range [0, 1]\n",
    "    x_batch = torch.flatten(x_batch / 255, start_dim=1)\n",
    "\n",
    "    y_hat_batch = network(x_batch)\n",
    "    _, prediction = torch.max(y_hat_batch, 1)\n",
    "    n_correct += (prediction == y_batch).sum().item()\n",
    "\n",
    "print(f'Accuracy = {(n_correct * 100) / task1_ts_samples.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 90.26004728132388\n"
     ]
    }
   ],
   "source": [
    "# test on Task 2\n",
    "n_batch = floor(task2_ts_samples.shape[0] / minibatch_sz)\n",
    "\n",
    "n_correct = 0    \n",
    "for b in range(n_batch):\n",
    "    x_batch = test_data.data[task2_ts_samples[(b*minibatch_sz):((b+1)*minibatch_sz)]]\n",
    "    y_batch = test_data.targets[task2_ts_samples[(b*minibatch_sz):((b+1)*minibatch_sz)]]\n",
    "\n",
    "    # flatten image before presenting to the network and normalize intensities to the range [0, 1]\n",
    "    x_batch = torch.flatten(x_batch / 255, start_dim=1)\n",
    "\n",
    "    y_hat_batch = network(x_batch)\n",
    "    _, prediction = torch.max(y_hat_batch, 1)\n",
    "    n_correct += (prediction == (y_batch % 2)).sum().item()\n",
    "\n",
    "print(f'Accuracy = {(n_correct * 100) / task1_ts_samples.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Directions for further exploration**\n",
    "We will not share solutions for these questions.\n",
    "\n",
    "**Q1**: How does the proportion of samples saved for replay affect the model's performance?\n",
    "\n",
    "**Q2**: Use replay to train the nentwork on more than two tasks. What is the impact of replay on the memory used by your models? Note that replay-based approach requires that you save the replay samples from previous task forever. This implies that the memory required to store samples contributes to your models memory footprint.\n",
    "\n",
    "**Q3**: Can we chose replay samples more smartly so that we generate maximal impact while using minimal memory? For instance, can you use the network's prediction on a given task to identify samples stored for replay?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
